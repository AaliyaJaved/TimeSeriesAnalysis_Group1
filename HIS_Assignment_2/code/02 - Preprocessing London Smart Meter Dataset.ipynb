{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33659962-9a0b-4cac-8d19-3184163cc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the working directory to the root\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce03d17-9b58-44ba-8aed-eb016815bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import os\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tqdm as tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "np.random.seed()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47449e71-2fb2-43f6-92cc-9bb0190c098f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading data to pandas dataframe\n",
    "\n",
    "Let's use London Smart Meters dataset from Kaggle for the current exercise. The source of the data is from London Data Store and Jean-Michel D. has augumented the dataset with a few more details and packaged it as a Kaggle Dataset.\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "London Data Store, free and open data-sharing portal, provided this dataset, which was collected and enriched by Jean-Michel D and uploaded on Kaggle @ https://www.kaggle.com/jeanmidev/smart-meters-in-london. \n",
    "The dataset has energy consumption readings for a sample of 5,567 London Households that took part in the UK Power Networks led Low Carbon London project between November 2011 and February 2014. Readings were taken at half hourly intervals. Some metadata about the households are also available as part of the dataset.\n",
    "\n",
    "## Data Wrangling\n",
    "The Kaggle dataset also has the time series data preprocessed on a daily level and combined all the separate files, etc. But letâ€™s ignore those files and start with the raw files, which is in the hhblock_dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363d00d-7f4d-494a-8317-938e30a66590",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"imgs/chapter_2\", exist_ok=True)\n",
    "#Add your local machine database path here\n",
    "source_data = Path(\"C:/Users/zain.hanif/Desktop/HIS Project/Modern-Time-Series-Forecasting-with-Python/data/london_smart_meters/\")\n",
    "print(source_data)\n",
    "block_data_path = source_data/\"hhblock_dataset\"/\"hhblock_dataset\"\n",
    "#Command to check if the directory exist in your system or not\n",
    "print(block_data_path)\n",
    "print(block_data_path.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ed620-92a0-4539-a3c0-f23fe52d9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert block_data_path.is_dir(), \"Please check if the dataset has been downloaded properly. Refer to the Preface of the book or the Readme in the repo for expected data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a701f-df4e-4b25-b980-aebb0c57361f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Converting the half hourly block level dataset into a time series data \n",
    "\n",
    "Let's pick one block and see how we can transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1bfe9-fd32-4f13-8c53-2a5d12e66df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_1 = pd.read_csv(block_data_path/\"block_0.csv\", parse_dates=False)\n",
    "\n",
    "block_1['day'] = pd.to_datetime(block_1['day'], yearfirst=True)\n",
    "\n",
    "block_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc08a06-725f-45e4-ac22-d018a7d5317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check End Dates of all time series\n",
    "block_1.groupby(\"LCLid\")['day'].max().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cadd15d-1d8a-4779-b64b-db8ba612b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = None\n",
    "for f in tqdm(block_data_path.glob(\"*.csv\")):\n",
    "    df = pd.read_csv(f, parse_dates=False)\n",
    "    df['day'] = pd.to_datetime(df['day'], yearfirst=True)\n",
    "    if max_date is None:\n",
    "        max_date = df['day'].max()\n",
    "    else:\n",
    "        if df['day'].max()>max_date:\n",
    "            max_date = df['day'].max()\n",
    "print(f\"Max Date across all blocks: {max_date}\")\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1271f2-201e-4544-95b7-f006bab2594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the dataframe into the long form with hour blocks along the rows\n",
    "block_1 = block_1.set_index(['LCLid', \"day\"]).stack().reset_index().rename(columns={\"level_2\": \"hour_block\", 0: \"energy_consumption\"})\n",
    "#Creating a numerical hourblock column\n",
    "block_1['offset'] = block_1['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "\n",
    "block_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4e040-ea80-4353-939d-4fd7ff2a2880",
   "metadata": {},
   "source": [
    "#### Compact Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19551b7-b632-4865-90af-2abf172da3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_compact(x):\n",
    "    start_date = x['day'].min()\n",
    "    name = x['LCLid'].unique()[0]\n",
    "    ### Fill missing dates with NaN ###\n",
    "    # Create a date range from  min to max\n",
    "    dr = pd.date_range(start=x['day'].min(), end=max_date, freq=\"1D\")\n",
    "    # Add hh_0 to hh_47 to columns and with some unstack magic recreating date-hh_x combinations\n",
    "    dr = pd.DataFrame(columns=[f\"hh_{i}\" for i in range(48)], index=dr).unstack().reset_index()\n",
    "    # renaming the columns\n",
    "    dr.columns = [\"hour_block\", \"day\", \"_\"]\n",
    "    # left merging the dataframe to the standard dataframe\n",
    "    # now the missing values will be left as NaN\n",
    "    dr = dr.merge(x, on=['hour_block','day'], how='left')\n",
    "    # sorting the rows\n",
    "    dr.sort_values(['day',\"offset\"], inplace=True)\n",
    "    # extracting the timeseries array\n",
    "    ts = dr['energy_consumption'].values\n",
    "    len_ts = len(ts)\n",
    "    return start_date, name, ts, len_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbbf9b-6a29-4289-9ae0-2d311388a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_block_compact(block_df, freq=\"30min\", ts_identifier=\"series_name\", value_name=\"series_value\"):\n",
    "    grps = block_df.groupby('LCLid')\n",
    "    all_series = []\n",
    "    all_start_dates = []\n",
    "    all_names = []\n",
    "    all_data = {}\n",
    "    all_len = []\n",
    "    for idx, df in tqdm(grps, leave=False):\n",
    "        start_date, name, ts, len_ts = preprocess_compact(df)\n",
    "        all_series.append(ts)\n",
    "        all_start_dates.append(start_date)\n",
    "        all_names.append(name)\n",
    "        all_len.append(len_ts)\n",
    "\n",
    "    all_data[ts_identifier] = all_names\n",
    "    all_data['start_timestamp'] = all_start_dates\n",
    "    all_data['frequency'] = freq\n",
    "    all_data[value_name] = all_series\n",
    "    all_data['series_length'] = all_len\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "block1_compact = load_process_block_compact(block_1, freq=\"30min\", ts_identifier=\"LCLid\", value_name=\"energy_consumption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768098c-cb7b-42ff-9ff2-b09589953218",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_compact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22221006-26c7-4c11-92d4-7fbe6d4ec89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(block1_compact.memory_usage(deep=True))\n",
    "print(f\"Total: {block1_compact.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e178ac-5e36-466a-bc4c-a068c3fe89c3",
   "metadata": {},
   "source": [
    "#### Expanded Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf102637-d9c8-4869-9d58-4e1ace5de1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_expanded(x):\n",
    "    start_date = x['day'].min()\n",
    "    ### Fill missing dates with NaN ###\n",
    "    # Create a date range from  min to max\n",
    "    dr = pd.date_range(start=x['day'].min(), end=x['day'].max(), freq=\"1D\")\n",
    "    # Add hh_0 to hh_47 to columns and with some unstack magic recreating date-hh_x combinations\n",
    "    dr = pd.DataFrame(columns=[f\"hh_{i}\" for i in range(48)], index=dr).unstack().reset_index()\n",
    "    # renaming the columns\n",
    "    dr.columns = [\"hour_block\", \"day\", \"_\"]\n",
    "    # left merging the dataframe to the standard dataframe\n",
    "    # now the missing values will be left as NaN\n",
    "    dr = dr.merge(x, on=['hour_block','day'], how='left')\n",
    "    dr['series_length'] = len(dr)\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b89d9b-a6bc-413e-aef0-b33dd2a69f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_block_expanded(block_df, freq=\"30min\"):\n",
    "    grps = block_df.groupby('LCLid')\n",
    "    all_series = []\n",
    "    for idx, df in tqdm(grps, leave=False):\n",
    "        ts = preprocess_expanded(df)\n",
    "        all_series.append(ts)\n",
    "\n",
    "    block_df = pd.concat(all_series)\n",
    "    # Recreate Offset because there would be null rows now\n",
    "    block_df['offset'] = block_df['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "    # Creating a datetime column with the date | Will take some time because operation is not vectorized\n",
    "    block_df['timestamp'] = block_df['day'] + block_df['offset']*30*pd.offsets.Minute()\n",
    "    block_df['frequency'] = freq\n",
    "    block_df.sort_values([\"LCLid\",\"timestamp\"], inplace=True)\n",
    "    block_df.drop(columns=[\"_\", \"hour_block\", \"offset\", \"day\"], inplace=True)\n",
    "    return block_df\n",
    "#     del all_series\n",
    "block1_expanded = load_process_block_expanded(block_1, freq=\"30min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46c20-c802-4b28-a18f-0fc3bfe3fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ea3c3-94e4-4093-b9cf-77a8bc35198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(block1_expanded.memory_usage())\n",
    "print(f\"Total: {block1_expanded.memory_usage().sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76246e-bdc6-4ec9-b860-27da4071edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del block1_expanded, block_1, block1_compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eaa201-7292-4f33-8ff2-d84e5b767fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading and combining all the block data into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c68f5b-fe77-42a4-b375-ac6ef8f23862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Use this if in a Jupyter environment\n",
    "block_df_l = []\n",
    "for file in tqdm(sorted(list(block_data_path.glob(\"*.csv\"))), desc=\"Processing Blocks..\"):\n",
    "    block_df = pd.read_csv(file, parse_dates=False)\n",
    "    block_df['day'] = pd.to_datetime(block_df['day'], yearfirst=True)\n",
    "    # Taking only from 2015-01-01 instead of 2012\n",
    "    block_df = block_df.loc[block_df['day']>=\"2012-01-01\"]\n",
    "    #Reshaping the dataframe into the long form with hour blocks along the rows\n",
    "    block_df = block_df.set_index(['LCLid', \"day\"]).stack().reset_index().rename(columns={\"level_2\": \"hour_block\", 0: \"energy_consumption\"})\n",
    "    #Creating a numerical hourblock column\n",
    "    block_df['offset'] = block_df['hour_block'].str.replace(\"hh_\", \"\").astype(int)\n",
    "    block_df_l.append(load_process_block_compact(block_df, freq=\"30min\", ts_identifier=\"LCLid\", value_name=\"energy_consumption\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334663-867e-448e-82e3-8d796872b2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hhblock_df = pd.concat(block_df_l)\n",
    "del block_df_l\n",
    "display(hhblock_df.memory_usage(deep=True))\n",
    "print(f\"Total: {hhblock_df.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395b1b5-4f51-4ea4-979c-92720896e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329ca8-2d49-4ebd-a2dc-05399a4a616f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merging additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d04aba-d859-489e-81e5-dcef0caa0d96",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Household information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e77aa-99c0-407e-9ab6-9a59bc56bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "household_info = pd.read_csv(source_data/\"informations_households.csv\")\n",
    "household_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047a646-7883-40d5-8423-bd469b0ab326",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhblock_df = hhblock_df.merge(household_info, on='LCLid', validate=\"one_to_one\")\n",
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa90fb-7fb6-4a08-a631-9a6d8ef86f40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Weather and Bank Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73358282-4bf3-43ae-97f8-f96f4310ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_holidays = pd.read_csv(source_data/\"uk_bank_holidays.csv\", parse_dates=False)\n",
    "bank_holidays['Bank holidays'] = pd.to_datetime(bank_holidays['Bank holidays'], yearfirst=True)\n",
    "bank_holidays.set_index(\"Bank holidays\", inplace=True)\n",
    "bank_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d64c6-b29b-4f6e-8c89-793fe83ecc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reindex on standard date range\n",
    "bank_holidays = bank_holidays.resample(\"30min\").asfreq()\n",
    "bank_holidays = bank_holidays.groupby(bank_holidays.index.date).ffill().fillna(\"NO_HOLIDAY\")\n",
    "bank_holidays.index.name=\"datetime\"\n",
    "bank_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27a978-802a-4068-8c0c-eb3237886241",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hourly = pd.read_csv(source_data/\"weather_hourly_darksky.csv\", parse_dates=False)\n",
    "weather_hourly['time'] = pd.to_datetime(weather_hourly['time'], yearfirst=True)\n",
    "weather_hourly.set_index(\"time\", inplace=True)\n",
    "weather_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10258a10-ca32-4e23-b1ec-eb430cee2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling at 30min and forward fill\n",
    "weather_hourly = weather_hourly.resample(\"30min\").ffill()\n",
    "weather_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034ec41-6e2b-42fd-ac0f-6532c93c436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_weather_holidays(row):\n",
    "    date_range = pd.date_range(row['start_timestamp'], periods=row['series_length'], freq=row['frequency'])\n",
    "    std_df = pd.DataFrame(index=date_range)\n",
    "    #Filling Na iwth NO_HOLIDAY cause rows before earliers holiday will be NaN\n",
    "    holidays = std_df.join(bank_holidays, how=\"left\").fillna(\"NO_HOLIDAY\")\n",
    "    weather = std_df.join(weather_hourly, how='left')\n",
    "    assert len(holidays)==row['series_length'], \"Length of holidays should be same as series length\"\n",
    "    assert len(weather)==row['series_length'], \"Length of weather should be same as series length\"\n",
    "    row['holidays'] = holidays['Type'].values\n",
    "    for col in weather:\n",
    "        row[col] = weather[col].values\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272753d-a258-4292-af01-691e538c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "hhblock_df = hhblock_df.progress_apply(map_weather_holidays, axis=1)\n",
    "\n",
    "hhblock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d9e6f-00ce-4984-979d-1d0c44416b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del block_df, weather_hourly, bank_holidays, household_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a315b-3f17-4c82-801a-2c96da54ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hhblock_df.memory_usage(deep=True))\n",
    "print(f\"Total: {hhblock_df.memory_usage(deep=True).sum()/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf0dba-a6a3-44df-bde0-44641466a322",
   "metadata": {},
   "source": [
    "### Saving the file on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080995b2-4895-4944-9f31-b5463d7935d0",
   "metadata": {},
   "source": [
    "Saving the entire dataset in a ts file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b67d9-3007-4eb0-9aeb-a21a3cc0161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.makedirs(\"data/london_smart_meters/preprocessed\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627cbba-e116-4aa4-8be0-a72d1a5181c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a long time to finish. Comment out and execute only if needed\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\zain.hanif\\Desktop\\HIS Project\\Modern-Time-Series-Forecasting-with-Python')\n",
    "\n",
    "from src.utils.data_utils import write_compact_to_ts\n",
    "write_compact_to_ts(hhblock_df,\n",
    "       static_columns = ['LCLid', 'start_timestamp', 'frequency','series_length', 'stdorToU', 'Acorn', 'Acorn_grouped', 'file'], \n",
    "       time_varying_columns = ['energy_consumption', 'holidays', 'visibility', 'windBearing', 'temperature', 'dewPoint',\n",
    "                              'pressure', 'apparentTemperature', 'windSpeed', 'precipType', 'icon','humidity', 'summary'],\n",
    "       filename=f\"data/london_smart_meters/preprocessed/london_smart_meters_merged.ts\",\n",
    "       sep=\";\",\n",
    "      chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616395a6-8d70-44ec-aa8f-36293cdebcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the LCLid - Acorn map as a pickle to be used later\n",
    "hhblock_df[['LCLid',\"file\", \"Acorn_grouped\"]].to_pickle(f\"data/london_smart_meters/preprocessed/london_smart_meters_lclid_acorn_map.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c637a5-99ac-421a-9562-a25ae0d71f13",
   "metadata": {},
   "source": [
    "Saving blocks in 8 chunks as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3f53c-11e4-4dcc-9449-a043419f7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the blocks into 8 chunks\n",
    "blocks = [f\"block_{i}\" for i in range(111)]\n",
    "\n",
    "n_chunks= 8\n",
    "split_blocks = [blocks[i:i + n_chunks] for i in range(0, len(blocks), n_chunks)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6235e-73cb-4c6e-a320-d5d77776f6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Writing each chunk to disk\n",
    "for blk in tqdm(split_blocks):\n",
    "    df = hhblock_df.loc[hhblock_df.file.isin(blk)]\n",
    "    blk = [int(b.replace(\"block_\",\"\")) for b in blk]\n",
    "    block_str = f\"block_{min(blk)}-{max(blk)}\"\n",
    "    df.to_parquet(f\"data/london_smart_meters/preprocessed/london_smart_meters_merged1_{block_str}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "30fe1d27e07bcd39a201a380e0896b1a8ddc5b4c0f1bff527b499634c2e361cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
